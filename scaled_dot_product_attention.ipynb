{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc32add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Followed blog here : https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SingleHeadedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, wQ, wK, wV) -> None:\n",
    "        super().__init__()\n",
    "        self.wQ = nn.Parameter(wQ, requires_grad=True)\n",
    "        self.wK = nn.Parameter(wK, requires_grad=True)\n",
    "        self.wV = nn.Parameter(wV, requires_grad=True)\n",
    "\n",
    "    def forward(self, query, key, value) -> None:\n",
    "        \"\"\"lets assume dimension of Q,K,V is same i.e == N, in reality query and key should always be of same dimension\n",
    "            N : length of sequence\n",
    "            Q : (N, Dq)\n",
    "            K : (N, Dk)\n",
    "            V : (N, Dv)\n",
    "            wQ :(Dq, D)\n",
    "            wK :(Dk, D)  \n",
    "            wV :(Dv, D) dimensions of wV need not be same as wQ, wK. Useful for applying cross attention\n",
    "        \"\"\"\n",
    "\n",
    "        # IMPLEMENT OUTPUT = SOFTMAX(WQ * WK.T / d**0.5) * Values\n",
    "\n",
    "        # (N, Dq) @ (Dq, D)  -> N * D\n",
    "        key_transformed = torch.matmul(key, self.wK)\n",
    "        #print(\"keys\", key_transformed)\n",
    "        # (N, Dv) @ (Dv, D)  -> N * D\n",
    "        value_transformed = torch.matmul(value, self.wV)\n",
    "       # print(\"values\", value_transformed)\n",
    "        # (N, Dq) @ (Dq, D)  -> N * D\n",
    "        query_transformed = torch.matmul(query, self.wQ)\n",
    "        #print(\"queries\", query_transformed)\n",
    "\n",
    "        # (N * D) @ (D * N) -> N * N\n",
    "        scores = torch.matmul(query_transformed, key_transformed.mT)  # /len(query)**0.5\n",
    "        #print(scores)\n",
    "\n",
    "        softmax_scores = torch.softmax(scores, dim=-1)\n",
    " \n",
    "       \n",
    "        # (N * N) @ (N * D) -> N * D\n",
    "        attended_values = torch.matmul(softmax_scores, value_transformed)\n",
    "\n",
    "        return attended_values, softmax_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18b4c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testcase\n",
    "# N = 3, Dq,Dv,Dk = 4, D = 3\n",
    "\n",
    "x = [\n",
    "    [1, 0, 1, 0],  # Input 1\n",
    "    [0, 2, 0, 2],  # Input 2\n",
    "    [1, 1, 1, 1],  # Input 3\n",
    "]\n",
    "\n",
    "wK = [\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "wQ = [\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 1],\n",
    "]\n",
    "wV = [\n",
    "    [0, 2, 0],\n",
    "    [0, 3, 0],\n",
    "    [1, 0, 3],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "wQ, wK, wV = torch.Tensor(wQ), torch.Tensor(wK), torch.Tensor(wV)\n",
    "\n",
    "model = SingleHeadedAttention(wQ, wK, wV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4135f7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.9366, 6.6831, 1.5951],\n",
       "         [2.0000, 7.9640, 0.0540],\n",
       "         [1.9997, 7.7599, 0.3584]], grad_fn=<MmBackward0>),\n",
       " tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],\n",
       "         [6.0337e-06, 9.8201e-01, 1.7986e-02],\n",
       "         [2.9539e-04, 8.8054e-01, 1.1917e-01]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k , v = torch.Tensor(x), torch.Tensor(x), torch.Tensor(x)\n",
    "\n",
    "model(q, k , v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
