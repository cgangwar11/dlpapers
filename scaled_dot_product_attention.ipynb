{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SingleHeadedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, wQ, wK, wV) -> None:\n",
    "        super().__init__()\n",
    "        self.wQ = nn.Parameter(wQ, requires_grad=True)\n",
    "        self.wK = nn.Parameter(wK, requires_grad=True)\n",
    "        self.wV = nn.Parameter(wV, requires_grad=True)\n",
    "\n",
    "    def forward(self, query, key, value) -> None:\n",
    "        \"\"\"lets assume dimension of Q,K,V is same i.e == N, in reality query and key should always be of same dimension\n",
    "            N : number of examples \n",
    "            Q : (N, Dq)\n",
    "            K : (N, Dk)\n",
    "            V : (N, Dv)\n",
    "            wQ :(Dq, D)\n",
    "            wK :(Dk, D)  \n",
    "            wV :(Dv, D) dimensions of wV need not be same as wQ, wK. Useful for applying cross attention\n",
    "        \"\"\"\n",
    "\n",
    "        # IMPLEMENT OUTPUT = SOFTMAX(WQ * WK.T / d**0.5) * Values\n",
    "\n",
    "        # (N, Dq) @ (Dq, D)  -> N * D\n",
    "        key_transformed = torch.matmul(key, self.wK)\n",
    "        print(\"keys\", key_transformed)\n",
    "        # (N, Dv) @ (Dv, D)  -> N * D\n",
    "        value_transformed = torch.matmul(value, self.wV)\n",
    "        print(\"values\", value_transformed)\n",
    "        # (N, Dq) @ (Dq, D)  -> N * D\n",
    "        query_transformed = torch.matmul(query, self.wQ)\n",
    "        print(\"queries\", query_transformed)\n",
    "\n",
    "        # (N * D) @ (D * N) -> N * N\n",
    "        scores = torch.matmul(query_transformed, key_transformed.mT)  # /len(query)**0.5\n",
    "        print(scores)\n",
    "\n",
    "        softmax_scores = torch.softmax(scores, dim=-1)\n",
    " \n",
    "       \n",
    "        # (N * N) @ (N * D) -> N * D\n",
    "        attended_values = torch.matmul(softmax_scores, value_transformed)\n",
    "\n",
    "        return attended_values, softmax_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testcase\n",
    "# N = 3, Dq,Dv,Dk = 4, D = 3\n",
    "\n",
    "x = [\n",
    "    [1, 0, 1, 0],  # Input 1\n",
    "    [0, 2, 0, 2],  # Input 2\n",
    "    [1, 1, 1, 1],  # Input 3\n",
    "]\n",
    "\n",
    "wK = [\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 0],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "wQ = [\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 1],\n",
    "]\n",
    "wV = [\n",
    "    [0, 2, 0],\n",
    "    [0, 3, 0],\n",
    "    [1, 0, 3],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "wQ, wK, wV = torch.Tensor(wQ), torch.Tensor(wK), torch.Tensor(wV)\n",
    "\n",
    "model = SingleHeadedAttention(wQ, wK, wV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4135f7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys tensor([[0., 1., 1.],\n",
      "        [4., 4., 0.],\n",
      "        [2., 3., 1.]], grad_fn=<MmBackward0>)\n",
      "values tensor([[1., 2., 3.],\n",
      "        [2., 8., 0.],\n",
      "        [2., 6., 3.]], grad_fn=<MmBackward0>)\n",
      "queries tensor([[1., 0., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 1., 3.]], grad_fn=<MmBackward0>)\n",
      "tensor([[ 2.,  4.,  4.],\n",
      "        [ 4., 16., 12.],\n",
      "        [ 4., 12., 10.]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1.9366, 6.6831, 1.5951],\n",
       "         [2.0000, 7.9640, 0.0540],\n",
       "         [1.9997, 7.7599, 0.3584]], grad_fn=<MmBackward0>),\n",
       " tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],\n",
       "         [6.0337e-06, 9.8201e-01, 1.7986e-02],\n",
       "         [2.9539e-04, 8.8054e-01, 1.1917e-01]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k , v = torch.Tensor(x), torch.Tensor(x), torch.Tensor(x)\n",
    "\n",
    "model(q, k , v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use torch api with same input \n",
    "\n",
    "module = torch.nn.MultiheadAttention(embed_dim=4 , kdim=4, vdim=4 , num_heads=1, batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2f93d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2326, -0.1976,  0.3269, -0.2430],\n",
       "         [-0.2349, -0.1973,  0.3276, -0.2419],\n",
       "         [-0.2378, -0.1959,  0.3273, -0.2386]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[0.3400, 0.3260, 0.3340],\n",
       "         [0.3381, 0.3230, 0.3389],\n",
       "         [0.3424, 0.3208, 0.3368]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(q, k , v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c74e976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.q_proj_weight, module.v_proj_weight, module.k_proj_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9b4c6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.4270,  0.7082,  0.7191],\n",
       "         [-0.1559, -0.8517,  0.1958],\n",
       "         [ 0.4888,  0.9493,  0.0645]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.2847, -0.6150, -0.1386, -0.1009],\n",
       "         [-0.3014, -0.8046,  0.4053, -0.9241],\n",
       "         [ 0.9085, -0.8638,  0.2429, -0.4422]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.8045, -0.1455, -0.4198,  0.6208],\n",
       "         [ 0.0383,  0.2769, -0.9163,  0.8202],\n",
       "         [ 0.0949,  0.9156,  0.8680, -0.5306]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0146, -0.0544, -0.4000],\n",
       "         [ 0.2542,  0.3828,  0.3437],\n",
       "         [ 0.4194, -0.4564,  0.1957]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0.], requires_grad=True)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(module.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
